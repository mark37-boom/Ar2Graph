#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
arXiv å¤šå…³é”®è¯çˆ¬è™« + OpenAI æ™ºèƒ½ä½“ â†’ çŸ¥è¯†å›¾è°±ï¼ˆå…¨éƒ¨åŒ¹é…æ–‡ç« ï¼‰
----------------------------------------------------------------
1. çˆ¬å– arXiv æ‰€æœ‰å«å…³é”®è¯è®ºæ–‡ï¼ˆarticles_all.jsonï¼‰
2. ç”¨ OpenAI æŠ½å–ä¸‰å…ƒç»„ (head, relation, tail)
3. ç”Ÿæˆäº¤äº’å¼çŸ¥è¯†å›¾è°± knowledge_graph.html
"""
import argparse
import json
import os
import time
import datetime as dt
from typing import List, Dict, Optional, Tuple
from urllib.parse import urljoin, quote_plus
import difflib
import openai
import networkx as nx
from pyvis.network import Network
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import html

# ---------- é…ç½® ----------
BASE_URL = "https://arxiv.org"
SEARCH_TMP = "/search/?query={kw}&searchtype=all&start={start}"
HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; FuzzyBot/0.1)"}
SLEEP = 1
SAVE_JSON = "articles_all.json"
SAVE_MD = "articles_all.md"
DEFAULT_KEY = "Defect Detection on a Wind Turbine Blade"
DEFAULT_TITLE = " "
DEFAULT_ABSTRACT = ""
DEFAULT_PAGES = 1
MATCH_THRESHOLD = 0.12
FALLBACK_THRESHOLD = 0.02
FALLBACK_MAX_RESULTS = 30
MENTION_RELATION = "mentions"
INNOVATION_KEYWORDS = [
    "novel", "innovative", "proposed", "propose", "first", "new", "framework",
    "method", "approach", "architecture", "contribution", "breakthrough", "advance",
    "improve", "improved", "enhanced", "multi-modal", "multi modal", "ååŒ", "åˆ›æ–°",
    "æå‡º", "é¦–ä¸ª", "æ–°å‹", "å¤šæ¨¡æ€", "æ¡†æ¶", "æ–¹æ³•", "ç³»ç»Ÿ"
]
GRAPH_DESCRIPTION = (
    "è¯¥çŸ¥è¯†å›¾è°±ç”±ä¸‰ç±»èŠ‚ç‚¹ç»„æˆï¼šçº¢è‰²èŠ‚ç‚¹ä»£è¡¨è®ºæ–‡ï¼ˆåŒ…å«æ ‡é¢˜ã€æ‘˜è¦ä¸é“¾æ¥ï¼‰ï¼Œ"
    "è“è‰²èŠ‚ç‚¹ä»£è¡¨ä»è®ºæ–‡æ‘˜è¦ä¸­æŠ½å–çš„å®ä½“ï¼Œç´«è‰²èŠ‚ç‚¹ä»£è¡¨ä»è®ºæ–‡ä¸­æå–çš„é«˜é¢‘å…³é”®è¯ã€‚"
    "ç»¿è‰²è¾¹è¡¨ç¤ºè®ºæ–‡æåŠçš„å®ä½“ï¼Œè“è‰²è¾¹è¡¨ç¤ºå®ä½“ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œç´«è‰²è¾¹è¡¨ç¤ºè®ºæ–‡åŒ…å«çš„å…³é”®è¯ã€‚"
    "å¯ä»¥æ‹–æ‹½èŠ‚ç‚¹æŸ¥çœ‹å±€éƒ¨ç»“æ„ï¼Œæˆ–æ‚¬åœåœ¨èŠ‚ç‚¹/è¾¹ä¸ŠæŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ã€‚"
    "æ©™è‰²èŠ‚ç‚¹è¡¨ç¤ºå…·æœ‰è¾ƒé«˜åˆ›æ–°åº¦çš„è®ºæ–‡ã€‚"
)
BAILIAN_KEY = os.getenv("sk-acb35646d20348cea1fff58447e93430") or "sk-acb35646d20348cea1fff58447e93430"  # ç™¾ç‚¼æ§åˆ¶å°è·å–
BAILIAN_BASE = "https://dashscope.aliyuncs.com/compatible-mode/v1"   # å›ºå®šå…¼å®¹åœ°å€
MODEL_NAME = "qwen-turbo"                                            # å¯é€‰ qwen-
TRIPLES_FILE = "triples.json"
GRAPH_FILE = "knowledge_graph.html"
import re
# --------------------------


# ---------- å·¥å…· ----------
def clean_json(raw: str) -> str:
    # 1. å»æ‰æ§åˆ¶å­—ç¬¦
    raw = re.sub(r'[\x00-\x1f]', ' ', raw)
    # 2. æŠŠå•ä¸ªåæ–œæ æ›¿æ¢æˆåŒåæ–œæ ï¼ˆé˜²æ­¢æ— æ•ˆè½¬ä¹‰ï¼‰
    raw = raw.replace('\\', '\\\\')
    # 3. å†æŠŠ \\\\ è¿˜åŸæˆåˆæ³• \\\\ é¿å…è¿‡åº¦è½¬ä¹‰
    raw = raw.replace('\\\\\\\\', '\\\\')
    return raw

def save_json(path: str, data: List[Dict]) -> None:
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def save_md(path: str, data: List[Dict]) -> None:
    lines = ["| Rank | Title | Abstract | Link |",
             "|------|-------|----------|------|"]
    for rank, art in enumerate(data, 1):
        title = art["title"].replace("|", r"\|")
        abstract = art["abstract"][:150].replace("|", r"\|") + "..."
        link = f"[link]({art['url']})"
        lines.append(f"| {rank} | {title} | {abstract} | {link} |")
    with open(path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))


def build_keywords_from_text(title: str, abstract: str) -> List[str]:
    tokens = []
    for text in (title, abstract):
        if not text:
            continue
        tokens.extend([tok for tok in re.split(r"\s+", text.strip()) if tok])
    return tokens


def build_keywords_from_arg(key_arg: str) -> List[str]:
    if not key_arg:
        return []
    return [tok for tok in re.split(r"\s+", key_arg.strip()) if tok]


def compute_innovation_score(text: str) -> float:
    if not text:
        return 0.0
    tokens = normalize_tokens(text)
    if not tokens:
        return 0.0
    score = 0
    for kw in INNOVATION_KEYWORDS:
        normalized_kw = kw.lower()
        count = sum(1 for token in tokens if normalized_kw in token)
        score += count
    return score / len(tokens)


TOKEN_PATTERN = re.compile(r"[A-Za-z0-9\u4e00-\u9fff]+")


def normalize_tokens(text: str) -> List[str]:
    if not text:
        return []
    return [tok.lower() for tok in TOKEN_PATTERN.findall(text)]


def compute_match_score(title: str, abstract: str, keywords: List[str]) -> float:
    key_tokens = [tok.lower() for tok in keywords if tok]
    text_tokens = normalize_tokens(f"{title} {abstract}")
    if not key_tokens or not text_tokens:
        return 0.0
    key_set = set(key_tokens)
    text_set = set(text_tokens)
    overlap = len(key_set & text_set) / len(key_set)
    seq_score = difflib.SequenceMatcher(
        None, " ".join(key_tokens), " ".join(text_tokens)
    ).ratio()
    return 0.7 * overlap + 0.3 * seq_score


# ---------- å¤šå…³é”®è¯ OR åŒ¹é… ----------
def fuzzy_match(title: str, abstract: str, keywords: List[str], threshold: float = MATCH_THRESHOLD) -> bool:
    return compute_match_score(title, abstract, keywords) >= threshold


def similarity(title: str, abstract: str, keywords: List[str]) -> float:
    return compute_match_score(title, abstract, keywords)


# ---------- çˆ¬è™«åŸºç±» ----------
class BaseCrawler:
    """çˆ¬è™«åŸºç±»ï¼Œå®šä¹‰é€šç”¨çˆ¬è™«æ¥å£å’Œé€šç”¨æ–¹æ³•"""
    
    def __init__(self, base_url: str, headers: Dict[str, str] = None, sleep: float = 1.0):
        """
        åˆå§‹åŒ–çˆ¬è™«åŸºç±»
        
        Args:
            base_url: åŸºç¡€URL
            headers: HTTPè¯·æ±‚å¤´
            sleep: è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.base_url = base_url
        self.headers = headers or {"User-Agent": "Mozilla/5.0 (compatible; FuzzyBot/0.1)"}
        self.sleep = sleep
    
    def fetch_page(self, url: str, timeout: int = 20) -> str:
        """
        è·å–é¡µé¢å†…å®¹ï¼ˆé€šç”¨æ–¹æ³•ï¼‰
        
        Args:
            url: ç›®æ ‡URL
            timeout: è¶…æ—¶æ—¶é—´
            
        Returns:
            é¡µé¢HTMLå†…å®¹
        """
        print(f"[+] æŠ“å– {url}")
        resp = requests.get(url, headers=self.headers, timeout=timeout)
        resp.raise_for_status()
        time.sleep(self.sleep)
        return resp.text
    
    def parse_page(self, html: str, keywords: List[str], threshold: float) -> Tuple[List[Dict], List[Dict]]:
        """
        è§£æé¡µé¢å†…å®¹ï¼ˆå­ç±»éœ€å®ç°ï¼‰
        
        Args:
            html: é¡µé¢HTMLå†…å®¹
            keywords: å…³é”®è¯åˆ—è¡¨
            threshold: åŒ¹é…é˜ˆå€¼
            
        Returns:
            (åŒ¹é…çš„æ–‡ç« åˆ—è¡¨, å€™é€‰æ–‡ç« åˆ—è¡¨)
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° parse_page æ–¹æ³•")
    
    def build_search_url(self, keywords: List[str], page: int) -> str:
        """
        æ„å»ºæœç´¢URLï¼ˆå­ç±»éœ€å®ç°ï¼‰
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            page: é¡µç 
            
        Returns:
            æœç´¢URL
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° build_search_url æ–¹æ³•")
    
    def compute_similarity(self, title: str, abstract: str, keywords: List[str]) -> float:
        """
        è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆé€šç”¨æ–¹æ³•ï¼‰
        
        Args:
            title: æ–‡ç« æ ‡é¢˜
            abstract: æ–‡ç« æ‘˜è¦
            keywords: å…³é”®è¯åˆ—è¡¨
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•°
        """
        return compute_match_score(title, abstract, keywords)
    
    def crawl(self, keywords: List[str], pages: int, threshold: float = MATCH_THRESHOLD) -> List[Dict]:
        """
        çˆ¬å–æ–‡ç« ï¼ˆé€šç”¨æ–¹æ³•ï¼‰
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            pages: çˆ¬å–é¡µæ•°
            threshold: åŒ¹é…é˜ˆå€¼
            
        Returns:
            åŒ¹é…çš„æ–‡ç« åˆ—è¡¨
        """
        matched_articles = []
        fallback_candidates = []
        
        for p in range(pages):
            url = self.build_search_url(keywords, p)
            html = self.fetch_page(url)
            page_matches, page_candidates = self.parse_page(html, keywords, threshold)
            matched_articles.extend(page_matches)
            fallback_candidates.extend(page_candidates)
        
        matched_articles.sort(key=lambda x: x["score"], reverse=True)
        if not matched_articles:
            fallback_candidates.sort(key=lambda x: x["score"], reverse=True)
            matched_articles = fallback_candidates[:FALLBACK_MAX_RESULTS]
        
        return matched_articles


# ---------- arXiv çˆ¬è™«å®ç° ----------
class ArXivCrawler(BaseCrawler):
    """arXiv çˆ¬è™«ï¼Œç»§æ‰¿è‡ª BaseCrawler"""
    
    def __init__(self, base_url: str = BASE_URL, search_template: str = SEARCH_TMP, 
                 headers: Dict[str, str] = None, sleep: float = SLEEP):
        """
        åˆå§‹åŒ– arXiv çˆ¬è™«
        
        Args:
            base_url: arXiv åŸºç¡€URL
            search_template: æœç´¢URLæ¨¡æ¿
            headers: HTTPè¯·æ±‚å¤´
            sleep: è¯·æ±‚é—´éš”æ—¶é—´
        """
        super().__init__(base_url, headers, sleep)
        self.search_template = search_template
    
    def build_search_url(self, keywords: List[str], page: int) -> str:
        """æ„å»º arXiv æœç´¢URL"""
        kw_str = " ".join(keywords)
        start = page * 50
        query = quote_plus(kw_str)
        path = self.search_template.format(kw=query, start=start)
        return urljoin(self.base_url, path)
    
    def parse_page(self, html: str, keywords: List[str], threshold: float) -> Tuple[List[Dict], List[Dict]]:
        """è§£æ arXiv æœç´¢ç»“æœé¡µé¢"""
        soup = BeautifulSoup(html, "lxml")
        articles = []
        candidates = []
        
        for entry in soup.select("li.arxiv-result"):
            title_tag = entry.select_one("p.title")
            abs_tag = entry.select_one("p.abstract")
            link_tag = entry.select_one("a[href*='/abs/']")
            
            if not all([title_tag, abs_tag, link_tag]):
                continue
            
            title = title_tag.get_text(strip=True)
            abstract = abs_tag.get_text(strip=True)
            link = urljoin(self.base_url, link_tag["href"])
            score = self.compute_similarity(title, abstract, keywords)
            
            candidate = {
                "title": title,
                "abstract": abstract,
                "url": link,
                "score": score
            }
            candidates.append(candidate)
            
            if score >= threshold:
                articles.append(candidate)
        
        candidates.sort(key=lambda x: x["score"], reverse=True)
        return articles, candidates


# ---------- æ™ºèƒ½ä½“æŠ½å– ----------
def extract_triples(text: str, title: str = "", client: OpenAI = None) -> List[Dict]:
    if client is None:
        client = OpenAI(api_key=BAILIAN_KEY, base_url=BAILIAN_BASE, timeout=60)
    prompt = f"""
You are a knowledge graph extractor.  
Given a paper title and abstract, output **only** a JSON list of triples:  
[{{"head": "...", "relation": "...", "tail": "..."}}]

Title: {title}
Abstract: {text}
"""
    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    content = clean_json(response.choices[0].message.content.strip())
    try:
        return json.loads(content)
    except Exception as e:
        print("è§£æå¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨", e)
        return []


def extract_keywords(text: str, title: str = "", client: OpenAI = None) -> List[str]:
    """
    ä»æ–‡ç« æ ‡é¢˜å’Œæ‘˜è¦ä¸­æå–å…³é”®è¯
    
    Args:
        text: æ–‡ç« æ‘˜è¦
        title: æ–‡ç« æ ‡é¢˜
        client: OpenAIå®¢æˆ·ç«¯
        
    Returns:
        å…³é”®è¯åˆ—è¡¨
    """
    if client is None:
        client = OpenAI(api_key=BAILIAN_KEY, base_url=BAILIAN_BASE, timeout=60)
    prompt = f"""
You are a keyword extractor.  
Given a paper title and abstract, extract 3-5 most important keywords that represent the core research topics.
Output **only** a JSON list of keywords: ["keyword1", "keyword2", "keyword3"]

Title: {title}
Abstract: {text}
"""
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        content = clean_json(response.choices[0].message.content.strip())
        keywords = json.loads(content)
        if isinstance(keywords, list):
            return [str(kw).strip() for kw in keywords if kw]
        return []
    except Exception as e:
        print(f"å…³é”®è¯æå–å¤±è´¥: {e}")
        return []


def generate_research_trends(hot_keywords: List[Dict], client: OpenAI = None) -> str:
    """
    æ ¹æ®é«˜é¢‘å…³é”®è¯ç”Ÿæˆæœªæ¥ç ”ç©¶è¶‹åŠ¿è®¨è®º
    
    Args:
        hot_keywords: é«˜é¢‘å…³é”®è¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"keyword": "...", "frequency": 10}, ...]
        client: OpenAIå®¢æˆ·ç«¯
        
    Returns:
        ç ”ç©¶è¶‹åŠ¿è®¨è®ºæ–‡æœ¬
    """
    if client is None:
        client = OpenAI(api_key=BAILIAN_KEY, base_url=BAILIAN_BASE, timeout=60)
    
    keywords_str = ", ".join([f"{kw['keyword']} (å‡ºç°{kw['frequency']}æ¬¡)" for kw in hot_keywords[:10]])
    prompt = f"""
You are a research trend analyst. Based on the following hot keywords extracted from recent papers, 
analyze and discuss the future research trends and directions in this field.

Hot Keywords:
{keywords_str}

Please provide a comprehensive analysis in Chinese, covering:
1. Current research focus areas
2. Emerging trends
3. Potential future directions
4. Interdisciplinary opportunities

Output format: A well-structured discussion text (no JSON, just plain text).
"""
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"ç ”ç©¶è¶‹åŠ¿ç”Ÿæˆå¤±è´¥: {e}")
        return "ç ”ç©¶è¶‹åŠ¿åˆ†æç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚"

# ---------- æ„å»ºå›¾è°± ----------
def build_graph(articles: List[Dict], client: OpenAI = None) -> Tuple[nx.MultiDiGraph, Dict[str, int]]:
    """
    æ„å»ºçŸ¥è¯†å›¾è°±ï¼ŒåŒ…å«è®ºæ–‡ã€å®ä½“å’Œå…³é”®è¯èŠ‚ç‚¹
    
    Returns:
        (çŸ¥è¯†å›¾è°±, å…³é”®è¯é¢‘ç‡å­—å…¸)
    """
    G = nx.MultiDiGraph()
    triples = []
    entity_frequency = {}
    keyword_frequency = {}  # ç»Ÿè®¡å…³é”®è¯é¢‘ç‡

    def add_or_update_entity(name: str):
        if not name:
            return
        current = entity_frequency.get(name, 0) + 1
        entity_frequency[name] = current
        node_data = G.nodes.get(name, {})
        G.add_node(
            name,
            type="entity",
            weight=current,
            description=node_data.get("description", "")
        )

    def add_or_update_keyword(keyword: str):
        """æ·»åŠ æˆ–æ›´æ–°å…³é”®è¯èŠ‚ç‚¹"""
        if not keyword or len(keyword.strip()) < 2:
            return
        keyword = keyword.strip()
        current = keyword_frequency.get(keyword, 0) + 1
        keyword_frequency[keyword] = current
        node_data = G.nodes.get(keyword, {})
        G.add_node(
            keyword,
            type="keyword",
            weight=current,
            frequency=current
        )

    def add_weighted_edge(src: str, dst: str, relation: str, edge_type: str, source_article: Optional[str] = None):
        if not src or not dst:
            return
        edge_data = G.get_edge_data(src, dst, default={})
        for key, data in edge_data.items():
            if data.get("relation") == relation and data.get("edge_type") == edge_type:
                data["weight"] = data.get("weight", 1) + 1
                if source_article:
                    sources = data.setdefault("sources", [])
                    if source_article not in sources:
                        sources.append(source_article)
                return
        G.add_edge(
            src,
            dst,
            relation=relation,
            edge_type=edge_type,
            weight=1,
            sources=[source_article] if source_article else []
        )

    print("[*] å¼€å§‹æå–å…³é”®è¯å’Œæ„å»ºçŸ¥è¯†å›¾è°±...")
    for idx, art in enumerate(articles, 1):
        print(f"[*] å¤„ç†æ–‡ç«  {idx}/{len(articles)}: {art['title'][:50]}...")
        article_node = art["title"][:80]
        summary = art["abstract"][:200].replace("\n", " ")
        innovation_score = compute_innovation_score(art["abstract"])
        G.add_node(
            article_node,
            type="article",
            url=art["url"],
            summary=summary,
            full_title=art["title"],
            innovation_score=innovation_score
        )
        
        # æå–å…³é”®è¯
        keywords = extract_keywords(art["abstract"], art["title"], client=client)
        for keyword in keywords:
            add_or_update_keyword(keyword)
            # è¿æ¥è®ºæ–‡å’Œå…³é”®è¯
            add_weighted_edge(article_node, keyword, "contains_keyword", "keyword", source_article=article_node)
        
        # æå–ä¸‰å…ƒç»„
        extracted = extract_triples(art["abstract"], art["title"], client=client)
        triples.extend(extracted)
        for triple in extracted:
            if "head" not in triple or "relation" not in triple or "tail" not in triple:
                print("è·³è¿‡ä¸å®Œæ•´ä¸‰å…ƒç»„ï¼š", triple)
                continue
            head = triple["head"]
            relation = triple["relation"]
            tail = triple["tail"]
            add_or_update_entity(head)
            add_or_update_entity(tail)
            add_weighted_edge(head, tail, relation, "knowledge", source_article=article_node)
            add_weighted_edge(article_node, head, MENTION_RELATION, "mention")
            add_weighted_edge(article_node, tail, MENTION_RELATION, "mention")

    json.dump(triples, open(TRIPLES_FILE, "w", encoding="utf-8"), ensure_ascii=False, indent=2)
    return G, keyword_frequency


# ---------- å›¾è°±æ´å¯Ÿ ----------
def generate_graph_insights(graph: nx.MultiDiGraph, keyword_frequency: Dict[str, int] = None) -> Dict[str, List[Dict]]:
    summary = {
        "node_count": graph.number_of_nodes(),
        "edge_count": graph.number_of_edges()
    }
    article_nodes = []
    entity_nodes = []
    keyword_nodes = []
    
    for node, data in graph.nodes(data=True):
        deg = graph.degree(node, weight="weight")
        info = {
            "name": node,
            "type": data.get("type"),
            "degree": deg,
            "weight": data.get("weight", 1),
            "url": data.get("url"),
            "summary": data.get("summary", ""),
            "innovation_score": data.get("innovation_score", 0),
            "frequency": data.get("frequency", 0)
        }
        node_type = data.get("type")
        if node_type == "article":
            article_nodes.append(info)
        elif node_type == "keyword":
            keyword_nodes.append(info)
        else:
            entity_nodes.append(info)

    article_nodes_by_degree = sorted(
        article_nodes,
        key=lambda x: (x["degree"], x.get("innovation_score", 0)),
        reverse=True
    )
    article_nodes_by_innovation = sorted(
        article_nodes,
        key=lambda x: x.get("innovation_score", 0),
        reverse=True
    )
    entity_nodes.sort(key=lambda x: x["degree"], reverse=True)
    keyword_nodes.sort(key=lambda x: (x.get("frequency", 0), x["degree"]), reverse=True)

    knowledge_edges = [
        {
            "src": src,
            "dst": dst,
            "relation": data.get("relation"),
            "weight": data.get("weight", 1)
        }
        for src, dst, data in graph.edges(data=True)
        if data.get("edge_type") == "knowledge"
    ]
    innovation = None
    if knowledge_edges:
        innovation = max(knowledge_edges, key=lambda x: x["weight"])

    # å‡†å¤‡é«˜é¢‘å…³é”®è¯åˆ—è¡¨ï¼ˆç”¨äºç”Ÿæˆç ”ç©¶è¶‹åŠ¿ï¼‰
    hot_keywords = [
        {"keyword": kw["name"], "frequency": kw.get("frequency", 0)}
        for kw in keyword_nodes[:15]
    ]

    return {
        "summary": summary,
        "top_articles": article_nodes_by_degree[:3],
        "top_entities": entity_nodes[:3],
        "top_keywords": keyword_nodes[:10],
        "hot_keywords": hot_keywords,
        "innovation_edge": innovation,
        "top_innovations": article_nodes_by_innovation[:3]
    }


# ---------- HTML æ³¨å…¥ ----------
def inject_graph_description(graph_file: str, description: str, insights: Dict[str, List[Dict]], 
                             research_trends: str = "") -> None:
    try:
        with open(graph_file, "r", encoding="utf-8") as f:
            content = f.read()
    except FileNotFoundError:
        return
    if "kg-description" in content:
        return
    summary = insights.get("summary", {})
    top_articles = insights.get("top_articles", [])
    top_entities = insights.get("top_entities", [])
    top_keywords = insights.get("top_keywords", [])
    innovation = insights.get("innovation_edge")
    top_innovations = insights.get("top_innovations", [])

    def render_list(items: List[Dict], empty_text: str) -> str:
        if not items:
            return f"<li>{empty_text}</li>"
        rows = []
        for item in items:
            name = html.escape(item["name"])
            degree = item.get("degree", 0)
            extra = ""
            if item.get("type") == "article" and item.get("url"):
                extra = f'<a href="{html.escape(item["url"])}" target="_blank">åŸæ–‡é“¾æ¥</a>'
            innovation_badge = ""
            score = item.get("innovation_score", 0)
            if score and score > 0:
                innovation_badge = f"<span style='color:#f97316;'>Â· åˆ›æ–°åº¦ {score:.2f}</span>"
            frequency_badge = ""
            if item.get("type") == "keyword":
                freq = item.get("frequency", 0)
                frequency_badge = f"<span style='color:#9333ea;'>Â· å‡ºç°{freq}æ¬¡</span>"
            rows.append(f"<li><strong>{name}</strong>ï¼ˆè¿æ¥åº¦ {degree}ï¼‰ {innovation_badge} {frequency_badge} {extra}</li>")
        return "".join(rows)
    
    def render_keywords(keywords: List[Dict]) -> str:
        if not keywords:
            return "<p>æš‚æ— å…³é”®è¯æ•°æ®</p>"
        rows = []
        for kw in keywords[:10]:
            name = html.escape(kw["name"])
            freq = kw.get("frequency", 0)
            rows.append(f"<span style='display:inline-block;margin:4px 8px;padding:6px 12px;background:#f3e8ff;border-radius:6px;color:#7c3aed;'>{name} ({freq})</span>")
        return "<div style='margin-top:8px;'>" + "".join(rows) + "</div>"

    innovation_text = ""
    if innovation:
        innovation_text = (
            f"<p><strong>å›¾è°±åˆ›æ–°äº®ç‚¹ï¼š</strong>å®ä½“ <em>{html.escape(innovation['src'])}</em> "
            f"ä¸ <em>{html.escape(innovation['dst'])}</em> ä¹‹é—´çš„å…³ç³» "
            f"<em>{html.escape(innovation['relation'] or 'å…³è”')}</em> å‡ºç°é¢‘æ¬¡æœ€é«˜"
            f"ï¼ˆæƒé‡ {innovation['weight']}ï¼‰ï¼Œä»£è¡¨å½“å‰ä¸»é¢˜ä¸‹æœ€é‡è¦çš„è¯­ä¹‰è”ç»“ã€‚</p>"
        )

    desc_html = f"""
    <section id="kg-description" style="padding:20px 32px;margin:20px auto;max-width:1200px;
    background:#ffffff;border-radius:16px;box-shadow:0 12px 32px rgba(15,23,42,0.08);font-family:'Inter',Arial,sans-serif;">
        <h2 style="margin-top:0;font-size:24px;color:#111827;">çŸ¥è¯†å›¾è°±è¯´æ˜</h2>
        <p style="font-size:15px;line-height:1.7;color:#374151;">{description}</p>
        <div style="display:flex;gap:24px;flex-wrap:wrap;margin:12px 0;">
            <div style="flex:1;min-width:220px;">
                <h3 style="font-size:18px;color:#111827;margin-bottom:4px;">æ¦‚è§ˆ</h3>
                <p style="margin:4px 0;color:#4b5563;">èŠ‚ç‚¹ï¼š{summary.get("node_count", 0)} ä¸ª</p>
                <p style="margin:4px 0;color:#4b5563;">å…³ç³»ï¼š{summary.get("edge_count", 0)} æ¡</p>
            </div>
            <div style="flex:1;min-width:220px;">
                <h3 style="font-size:18px;color:#111827;margin-bottom:4px;">é‡è¦è®ºæ–‡èŠ‚ç‚¹</h3>
                <ul style="padding-left:18px;margin:0;color:#4b5563;">
                    {render_list(top_articles, "æš‚æ— æ•°æ®")}
                </ul>
            </div>
            <div style="flex:1;min-width:220px;">
                <h3 style="font-size:18px;color:#111827;margin-bottom:4px;">å…³é”®å®ä½“èŠ‚ç‚¹</h3>
                <ul style="padding-left:18px;margin:0;color:#4b5563;">
                    {render_list(top_entities, "æš‚æ— æ•°æ®")}
                </ul>
            </div>
            <div style="flex:1;min-width:220px;">
                <h3 style="font-size:18px;color:#111827;margin-bottom:4px;">åˆ›æ–°äº®ç‚¹è®ºæ–‡</h3>
                <ul style="padding-left:18px;margin:0;color:#4b5563;">
                    {render_list(top_innovations, "æš‚æ— æ•°æ®")}
                </ul>
            </div>
        </div>
        <div style="margin-top:20px;padding:16px;background:#f9fafb;border-radius:8px;">
            <h3 style="font-size:18px;color:#111827;margin-bottom:8px;">ğŸ”‘ é«˜é¢‘å…³é”®è¯</h3>
            {render_keywords(top_keywords)}
            <p style="margin-top:12px;font-size:14px;color:#6b7280;">ç´«è‰²èŠ‚ç‚¹è¡¨ç¤ºä»è®ºæ–‡ä¸­æå–çš„å…³é”®è¯ï¼ŒèŠ‚ç‚¹å¤§å°åæ˜ å…³é”®è¯å‡ºç°é¢‘ç‡ã€‚</p>
        </div>
        {innovation_text}
        {f'<div style="margin-top:24px;padding:20px;background:#fef3c7;border-left:4px solid #f59e0b;border-radius:8px;"><h3 style="font-size:18px;color:#111827;margin-top:0;margin-bottom:12px;">ğŸ”® æœªæ¥ç ”ç©¶è¶‹åŠ¿åˆ†æ</h3><div style="font-size:15px;line-height:1.8;color:#374151;white-space:pre-wrap;">{html.escape(research_trends)}</div></div>' if research_trends else ''}
        <ul style="padding-left:20px;font-size:14px;line-height:1.6;color:#4b5563;margin-top:20px;">
            <li>èŠ‚ç‚¹å¤§å°ä¸å‡ºç°é¢‘æ¬¡ç›¸å…³ï¼Œè¶Šå¤§ä»£è¡¨è¢«å¼•ç”¨æˆ–æåŠè¶Šå¤šã€‚</li>
            <li><strong>èŠ‚ç‚¹é¢œè‰²è¯´æ˜ï¼š</strong>çº¢è‰²=è®ºæ–‡ï¼Œè“è‰²=å®ä½“ï¼Œç´«è‰²=å…³é”®è¯ï¼Œæ©™è‰²=é«˜åˆ›æ–°åº¦è®ºæ–‡</li>
            <li>æ‚¬åœå¯æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ï¼›ç‚¹å‡»èŠ‚ç‚¹å¯å›ºå®šä½ç½®ï¼Œæ–¹ä¾¿åˆ†æå±€éƒ¨ç»“æ„ã€‚</li>
            <li>è‹¥å›¾è°±è¿‡äºå¯†é›†ï¼Œå¯ä½¿ç”¨å·¦ä¸Šè§’çš„å¯¼èˆªæŒ‰é’®æˆ–é¼ æ ‡æ»šè½®æ”¾å¤§ç¼©å°ã€‚</li>
        </ul>
    </section>
    """
    updated = content.replace("</body>", f"{desc_html}\n</body>", 1)
    with open(graph_file, "w", encoding="utf-8") as f:
        f.write(updated)


# ---------- å¯è§†åŒ– ----------
def visualize(graph: nx.MultiDiGraph, graph_file: str = "knowledge_graph.html"):
    net = Network(height="850px", width="100%", bgcolor="#f7f8fb", font_color="#1f1f1f")
    color_map = {"entity": "#6c6cff", "article": "#cc6666", "keyword": "#9333ea"}  # å…³é”®è¯ç”¨ç´«è‰²
    edge_color_map = {"knowledge": "#1f78b4", "mention": "#33a02c", "keyword": "#a855f7"}  # å…³é”®è¯è¾¹ç”¨ç´«è‰²
    net.set_options("""
    {
      "nodes": {
        "shape": "dot",
        "scaling": {
          "min": 10,
          "max": 45
        },
        "font": {
          "size": 16,
          "face": "Inter, Arial"
        },
        "borderWidth": 1
      },
      "edges": {
        "smooth": {
          "type": "dynamic",
          "roundness": 0.4
        },
        "color": {
          "inherit": false
        },
        "width": 1.5,
        "arrows": {
          "to": {
            "enabled": true,
            "scaleFactor": 0.7
          }
        }
      },
      "interaction": {
        "hover": true,
        "multiselect": true,
        "navigationButtons": true,
        "tooltipDelay": 120
      },
      "physics": {
        "enabled": true,
        "solver": "barnesHut",
        "barnesHut": {
          "gravitationalConstant": -22000,
          "springLength": 180,
          "springConstant": 0.045,
          "damping": 0.12
        },
        "stabilization": {
          "iterations": 250
        }
      }
    }
    """)
    for node, data in graph.nodes(data=True):
        print(f"æ·»åŠ èŠ‚ç‚¹ {node} ç±»å‹ {data.get('type', 'æœªçŸ¥ç±»å‹')}")
        weight = data.get("weight", 1)
        size = 15 + min(weight, 10)
        node_type = data.get("type", "unknown")
        tooltip_lines = [
            f"ç±»å‹: {node_type}",
            f"å‡ºç°æ¬¡æ•°: {weight}"
        ]
        if node_type == "article":
            tooltip_lines.append(f"åŸå§‹æ ‡é¢˜: {html.escape(data.get('full_title', node))}")
            summary = html.escape(data.get("summary", ""))
            if summary:
                tooltip_lines.append(f"æ‘˜è¦: {summary}...")
            url = data.get("url")
            if url:
                tooltip_lines.append(f"é“¾æ¥: {url}")
            innovation_score = data.get("innovation_score", 0)
            if innovation_score:
                tooltip_lines.append(f"åˆ›æ–°åº¦: {innovation_score:.2f}")
        elif node_type == "keyword":
            frequency = data.get("frequency", weight)
            tooltip_lines.append(f"å…³é”®è¯é¢‘ç‡: {frequency}")
            tooltip_lines.append("è¯¥å…³é”®è¯åœ¨è®ºæ–‡ä¸­å‡ºç°æ¬¡æ•°")
        else:
            description = html.escape(data.get("description", ""))
            if description:
                tooltip_lines.append(f"æè¿°: {description}")
        node_color = color_map.get(node_type, "gray")
        if node_type == "article" and data.get("innovation_score", 0) > 0:
            node_color = "#ff914d"
        net.add_node(
            node,
            label=node,
            color=node_color,
            value=size,
            title="<br>".join(tooltip_lines)
        )
    for src, dst, data in graph.edges(data=True):
        relation = data.get("relation", "æœªçŸ¥å…³ç³»")
        edge_type = data.get("edge_type", "knowledge")
        weight = data.get("weight", 1)
        print(f"æ·»åŠ è¾¹ {src} -> {dst} å…³ç³» {relation}")
        sources = data.get("sources") or []
        tooltip = [
            f"å…³ç³»: {html.escape(relation)}",
            f"ç±»å‹: {edge_type}",
            f"æƒé‡: {weight}"
        ]
        if sources:
            tooltip.append("æ¥æºæ–‡ç« : " + ", ".join(html.escape(s) for s in sources[:5]))
        # å…³é”®è¯è¾¹ä½¿ç”¨è™šçº¿
        is_dashed = edge_type in ["mention", "keyword"]
        net.add_edge(
            src,
            dst,
            title="<br>".join(tooltip),
            color=edge_color_map.get(edge_type, "#555555"),
            value=weight,
            width=1 + min(weight, 5),
            dashes=is_dashed,
            smooth=is_dashed
        )
    net.write_html(graph_file)


# ---------- å…¥å£ ---------
def main():

    client = OpenAI(
        api_key=BAILIAN_KEY,
        base_url=BAILIAN_BASE,
        timeout=60)
    parser = argparse.ArgumentParser(description="çˆ¬è™« + æ™ºèƒ½ä½“çŸ¥è¯†å›¾è°±")
    parser.add_argument("--key", default=DEFAULT_KEY,
                        help="ç›´æ¥è¾“å…¥æ£€ç´¢å…³é”®è¯ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰ï¼›é»˜è®¤ %(default)s")
    parser.add_argument("--title", default=DEFAULT_TITLE,
                        help="ç”¨äºæ£€ç´¢çš„è®ºæ–‡æ ‡é¢˜ï¼ˆå¯é€‰ï¼Œä¸æ‘˜è¦æ­é…ï¼‰")
    parser.add_argument("--abstract", default=DEFAULT_ABSTRACT,
                        help="ç”¨äºæ£€ç´¢çš„è®ºæ–‡æ‘˜è¦å†…å®¹ï¼ˆå¯é€‰ï¼Œä¸æ ‡é¢˜æ­é…ï¼‰")
    parser.add_argument("--pages", type=int, default=DEFAULT_PAGES,
                        help="ç¿»é¡µæ¬¡æ•°ï¼›é»˜è®¤ %(default)s")
    args = parser.parse_args()

    keywords = build_keywords_from_arg(args.key)
    if not keywords:
        keywords = build_keywords_from_text(args.title, args.abstract)
    if not keywords:
        parser.error("è¯·æä¾›å…³é”®è¯ï¼Œæˆ–æä¾›è®ºæ–‡æ ‡é¢˜ä¸æ‘˜è¦ç”¨äºæ£€ç´¢")

    # 1. çˆ¬å–ï¼ˆä½¿ç”¨ ArXivCrawlerï¼‰
    crawler = ArXivCrawler()
    articles = crawler.crawl(keywords, args.pages, MATCH_THRESHOLD)
    if not articles:
        print(f"[!] æœªåŒ¹é…åˆ°æ–‡ç« ï¼Œå°è¯•å°†é˜ˆå€¼é™ä½åˆ° {FALLBACK_THRESHOLD} é‡æ–°æœç´¢")
        articles = crawler.crawl(keywords, args.pages, FALLBACK_THRESHOLD)
    for art in articles:
        art.pop("score", None)
    save_json(SAVE_JSON, articles)
    save_md(SAVE_MD, articles)
    print(f"[âˆš] å…± {len(articles)} ç¯‡æ–‡ç« å·²ä¿å­˜")

    # 2. æ„å»ºå›¾è°±
    print("å¼€å§‹æ„å»ºå›¾è°±ï¼ˆ:")
    top10 = articles[:10]
    graph, keyword_frequency = build_graph(top10, client)
    print("graphåˆ¶ä½œä¸­")
    visualize(graph)
    
    # 3. ç”Ÿæˆå›¾è°±æ´å¯Ÿå’Œç ”ç©¶è¶‹åŠ¿
    print("[*] ç”Ÿæˆå›¾è°±æ´å¯Ÿ...")
    insights = generate_graph_insights(graph, keyword_frequency)
    
    # 4. æ ¹æ®é«˜é¢‘å…³é”®è¯ç”Ÿæˆæœªæ¥ç ”ç©¶è¶‹åŠ¿è®¨è®º
    hot_keywords = insights.get("hot_keywords", [])
    research_trends = ""
    if hot_keywords:
        print("[*] æ ¹æ®é«˜é¢‘å…³é”®è¯ç”Ÿæˆæœªæ¥ç ”ç©¶è¶‹åŠ¿åˆ†æ...")
        research_trends = generate_research_trends(hot_keywords, client)
    
    # 5. æ³¨å…¥æè¿°å’Œç ”ç©¶è¶‹åŠ¿åˆ°HTML
    inject_graph_description(GRAPH_FILE, GRAPH_DESCRIPTION, insights, research_trends)
    
    print(f"[âˆš] çŸ¥è¯†å›¾è°±å·²ç”Ÿæˆï¼š{GRAPH_FILE}")
    print(f"[âˆš] ä¸‰å…ƒç»„å·²ä¿å­˜ï¼š{TRIPLES_FILE}")
    if hot_keywords:
        print(f"[âˆš] é«˜é¢‘å…³é”®è¯ç»Ÿè®¡å®Œæˆï¼Œå…± {len(hot_keywords)} ä¸ªå…³é”®è¯")
        print(f"[âˆš] ç ”ç©¶è¶‹åŠ¿åˆ†æå·²ç”Ÿæˆ")


if __name__ == "__main__":
    main()